[core]
dags_folder = /home/airflow/gcs/dags  # Or your actual DAGs folder path

[dataproc]
environment_type = dev
local_markets = es
project_id = project-{local_markets}-type-{environment_type}
region = europe-west1
zone = europe-west1-b
master_machine_type = e2-standard-32
master_disk_size = 800
worker_machine_type = e2-standard-32
worker_disk_size = 800
num_workers = 20
num_preemptible_workers = 0
idle_delete_ttl = 14400
image_version = 1.5.53-debian10
storage_bucket = bucket-{local_markets}-type-{environment_type}
service_account = sa@project-name.iam.gserviceaccount.com
subnetwork_uri = projects/project-{local_markets}-type-{environment_type}/regions/europe-west1/subnetworks/your-subnetwork-name
spark_config_jar = gs://project-{local_markets}-type-{environment_type}/spark-xml/spark-xml_2.12-0.14.0.jar
dataproc_pyspark_jars = gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar, {spark_config_jar}
labels = {"cluster-goal": "dev", "use-case": "technology", "cluster-owner": "eilinlux-gmail_com", "team-tag": "DataEngineer"}
tags = allow-internal-dataproc-proda, allow-ssh-from-management-zone, allow-ssh-from-net-to-bastion
properties = core:fs.gs.implicit.dir.repair.enable=false, core:fs.gs.status.parallel.enable=true, dataproc:dataproc.logging.stackdriver.job.driver.enable=true, yarn:yarn.nodemanager.resource.memory-mb=118000, yarn:yarn.nodemanager.resource.cpu-vcores=30, yarn:yarn.nodemanager.pmem-check-enabled=false, yarn:yarn.nodemanager.vmem-check-enabled=false, dataproc:am.primary_only=true, spark:spark.yarn.am.cores=5, spark:spark.yarn.am.memory=40g
schedule_interval = 0 0 5 * * 

[DAG_DEFAULT]
start_date = 2015-12-01
email_on_failure = False
email_on_retry = False
retries = 0
retry_delay = 5  # Minutes

[airflow_variables]
local_markets = es
technologies = 4G,5G,2G
goal = technology-xml-parser
code_files_folder = {storage_bucket}/{goal}
main_python_file = {code_files_folder}/{goal.replace('-','_')}_main.py